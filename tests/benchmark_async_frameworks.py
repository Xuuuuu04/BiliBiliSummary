"""
å¼‚æ­¥æ¡†æ¶æ€§èƒ½åŸºå‡†æµ‹è¯•
å¯¹æ¯” Flask vs FastAPI çš„æ€§èƒ½å·®å¼‚
"""
import asyncio
import time
import statistics
import httpx
from typing import List, Dict
from dataclasses import dataclass
import json

# ========== é…ç½® ==========

FLASK_BASE_URL = "http://localhost:5000"
FASTAPI_BASE_URL = "http://localhost:5001"

# æµ‹è¯•å‚æ•°
CONCURRENT_REQUESTS = 50  # å¹¶å‘è¯·æ±‚æ•°
TOTAL_REQUESTS = 100      # æ€»è¯·æ±‚æ•°
TIMEOUT = 30              # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# ========== æ•°æ®ç»“æ„ ==========

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    framework: str
    endpoint: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_duration: float
    avg_latency: float
    min_latency: float
    max_latency: float
    p50_latency: float
    p95_latency: float
    p99_latency: float
    throughput: float  # req/s

    def to_dict(self) -> Dict:
        return {
            "framework": self.framework,
            "endpoint": self.endpoint,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "total_duration": round(self.total_duration, 3),
            "avg_latency": round(self.avg_latency * 1000, 2),  # ms
            "min_latency": round(self.min_latency * 1000, 2),
            "max_latency": round(self.max_latency * 1000, 2),
            "p50_latency": round(self.p50_latency * 1000, 2),
            "p95_latency": round(self.p95_latency * 1000, 2),
            "p99_latency": round(self.p99_latency * 1000, 2),
            "throughput": round(self.throughput, 2)
        }

    def print_summary(self):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"æ¡†æ¶: {self.framework} | ç«¯ç‚¹: {self.endpoint}")
        print(f"{'='*60}")
        print(f"æ€»è¯·æ±‚æ•°: {self.total_requests}")
        print(f"æˆåŠŸè¯·æ±‚: {self.successful_requests} ({self.successful_requests/self.total_requests*100:.1f}%)")
        print(f"å¤±è´¥è¯·æ±‚: {self.failed_requests}")
        print(f"æ€»è€—æ—¶: {self.total_duration:.2f}s")
        print(f"ååé‡: {self.throughput:.2f} req/s")
        print(f"\nå»¶è¿Ÿç»Ÿè®¡ (æ¯«ç§’):")
        print(f"  å¹³å‡: {self.avg_latency*1000:.2f}ms")
        print(f"  æœ€å°: {self.min_latency*1000:.2f}ms")
        print(f"  æœ€å¤§: {self.max_latency*1000:.2f}ms")
        print(f"  P50:  {self.p50_latency*1000:.2f}ms")
        print(f"  P95:  {self.p95_latency*1000:.2f}ms")
        print(f"  P99:  {self.p99_latency*1000:.2f}ms")
        print(f"{'='*60}\n")

# ========== æµ‹è¯•å‡½æ•° ==========

async def test_concurrent_requests(
    framework: str,
    base_url: str,
    endpoint: str = "/api/health",
    concurrent: int = CONCURRENT_REQUESTS,
    total: int = TOTAL_REQUESTS
) -> BenchmarkResult:
    """
    æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½

    Args:
        framework: æ¡†æ¶åç§° (Flask/FastAPI)
        base_url: API åŸºç¡€ URL
        endpoint: æµ‹è¯•ç«¯ç‚¹
        concurrent: å¹¶å‘æ•°
        total: æ€»è¯·æ±‚æ•°
    """
    print(f"\n[{framework}] å¼€å§‹å¹¶å‘æµ‹è¯•: {endpoint}")
    print(f"  -> å¹¶å‘æ•°: {concurrent}, æ€»è¯·æ±‚: {total}")

    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        latencies = []
        successful = 0
        failed = 0

        start_time = time.time()

        # åˆ†æ‰¹æ‰§è¡Œå¹¶å‘è¯·æ±‚
        for batch_start in range(0, total, concurrent):
            batch_size = min(concurrent, total - batch_start)
            batch_end = batch_start + batch_size

            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            tasks = []
            for i in range(batch_size):
                task = client.get(f"{base_url}{endpoint}")
                tasks.append(task)

            # æ‰§è¡Œå¹¶å‘è¯·æ±‚
            batch_start_time = time.time()
            try:
                responses = await asyncio.gather(*tasks, return_exceptions=True)

                for response in responses:
                    if isinstance(response, Exception):
                        failed += 1
                    elif response.status_code == 200:
                        successful += 1
                        latency = time.time() - batch_start_time
                        latencies.append(latency)
                    else:
                        failed += 1

            except Exception as e:
                print(f"  âŒ æ‰¹æ¬¡ {batch_start}-{batch_end} å¤±è´¥: {e}")
                failed += batch_size

        total_duration = time.time() - start_time

    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    if latencies:
        avg_latency = statistics.mean(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        p50_latency = statistics.median(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) > 20 else max_latency
        p99_latency = statistics.quantiles(latencies, n=100)[98] if len(latencies) > 100 else max_latency
    else:
        avg_latency = min_latency = max_latency = p50_latency = p95_latency = p99_latency = 0

    throughput = successful / total_duration if total_duration > 0 else 0

    result = BenchmarkResult(
        framework=framework,
        endpoint=endpoint,
        total_requests=total,
        successful_requests=successful,
        failed_requests=failed,
        total_duration=total_duration,
        avg_latency=avg_latency,
        min_latency=min_latency,
        max_latency=max_latency,
        p50_latency=p50_latency,
        p95_latency=p95_latency,
        p99_latency=p99_latency,
        throughput=throughput
    )

    result.print_summary()

    return result

async def test_sse_stream(
    framework: str,
    base_url: str,
    endpoint: str = "/api/analyze"
) -> BenchmarkResult:
    """
    æµ‹è¯• SSE æµå¼å“åº”æ€§èƒ½

    Args:
        framework: æ¡†æ¶åç§°
        base_url: API åŸºç¡€ URL
        endpoint: æµ‹è¯•ç«¯ç‚¹
    """
    print(f"\n[{framework}] å¼€å§‹ SSE æµå¼æµ‹è¯•: {endpoint}")

    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        latencies = []
        successful = 0
        failed = 0
        chunks_received = []

        start_time = time.time()

        # æµ‹è¯•å•ä¸ªæµå¼è¯·æ±‚
        try:
            request_start = time.time()
            ttfb = None  # é¦–å­—èŠ‚æ—¶é—´

            async with client.stream(
                "POST",
                f"{base_url}{endpoint}",
                json={"url": "https://www.bilibili.com/video/BV1xx411c7mD"}
            ) as response:
                if response.status_code == 200:
                    chunk_count = 0
                    async for chunk in response.aiter_text():
                        if ttfb is None:
                            ttfb = time.time() - request_start

                        if chunk.strip():
                            chunk_count += 1
                            chunks_received.append(chunk)

                        # é™åˆ¶è¯»å–çš„å—æ•°ï¼ˆé¿å…æµ‹è¯•æ—¶é—´è¿‡é•¿ï¼‰
                        if chunk_count >= 10:
                            break

                    successful = 1
                    latency = time.time() - request_start
                    latencies.append(latency)

                    print(f"  âœ… æµå¼è¯·æ±‚æˆåŠŸ")
                    print(f"     é¦–å­—èŠ‚æ—¶é—´(TTFB): {ttfb*1000:.2f}ms")
                    print(f"     æ¥æ”¶å—æ•°: {chunk_count}")
                else:
                    failed = 1
                    print(f"  âŒ æµå¼è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")

        except Exception as e:
            failed = 1
            print(f"  âŒ æµå¼è¯·æ±‚å¼‚å¸¸: {e}")

        total_duration = time.time() - start_time

    result = BenchmarkResult(
        framework=framework,
        endpoint=endpoint,
        total_requests=1,
        successful_requests=successful,
        failed_requests=failed,
        total_duration=total_duration,
        avg_latency=statistics.mean(latencies) if latencies else 0,
        min_latency=min(latencies) if latencies else 0,
        max_latency=max(latencies) if latencies else 0,
        p50_latency=statistics.median(latencies) if latencies else 0,
        p95_latency=statistics.quantiles(latencies, n=20)[18] if len(latencies) > 20 else (max(latencies) if latencies else 0),
        p99_latency=statistics.quantiles(latencies, n=100)[98] if len(latencies) > 100 else (max(latencies) if latencies else 0),
        throughput=successful / total_duration if total_duration > 0 else 0
    )

    return result

async def test_async_operations(
    framework: str,
    base_url: str
) -> Dict[str, BenchmarkResult]:
    """
    æµ‹è¯•å¼‚æ­¥æ“ä½œæ€§èƒ½å¯¹æ¯”

    Args:
        framework: æ¡†æ¶åç§°
        base_url: API åŸºç¡€ URL
    """
    print(f"\n[{framework}] å¼€å§‹å¼‚æ­¥æ“ä½œæ€§èƒ½æµ‹è¯•")

    results = {}

    # æµ‹è¯•1: åŒæ­¥æ“ä½œ
    print(f"\n  æµ‹è¯•1: åŒæ­¥æ“ä½œ (é˜»å¡ 1s)")
    start = time.time()
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{base_url}/api/sync/test")
            if response.status_code == 200:
                duration = time.time() - start
                print(f"  âœ… åŒæ­¥æ“ä½œè€—æ—¶: {duration:.2f}s")
                results["sync"] = duration
    except Exception as e:
        print(f"  âŒ åŒæ­¥æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        results["sync"] = None

    # æµ‹è¯•2: å¼‚æ­¥æ“ä½œ
    print(f"\n  æµ‹è¯•2: å¼‚æ­¥æ“ä½œ (await 1s)")
    start = time.time()
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{base_url}/api/async/test")
            if response.status_code == 200:
                duration = time.time() - start
                print(f"  âœ… å¼‚æ­¥æ“ä½œè€—æ—¶: {duration:.2f}s")
                results["async"] = duration
    except Exception as e:
        print(f"  âŒ å¼‚æ­¥æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        results["async"] = None

    # æµ‹è¯•3: å¹¶å‘æ“ä½œ
    print(f"\n  æµ‹è¯•3: å¹¶å‘æ“ä½œ (5ä¸ªä»»åŠ¡å¹¶è¡Œ)")
    start = time.time()
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{base_url}/api/concurrent/test")
            if response.status_code == 200:
                duration = time.time() - start
                data = response.json()
                print(f"  âœ… å¹¶å‘æ“ä½œè€—æ—¶: {duration:.2f}s")
                print(f"     æ‰§è¡Œä»»åŠ¡æ•°: {data.get('tasks')}")
                results["concurrent"] = duration
    except Exception as e:
        print(f"  âŒ å¹¶å‘æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        results["concurrent"] = None

    return results

# ========== å¯¹æ¯”åˆ†æ ==========

def compare_results(flask_result: BenchmarkResult, fastapi_result: BenchmarkResult):
    """
    å¯¹æ¯” Flask å’Œ FastAPI æ€§èƒ½å·®å¼‚

    Args:
        flask_result: Flask æµ‹è¯•ç»“æœ
        fastapi_result: FastAPI æµ‹è¯•ç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"æ€§èƒ½å¯¹æ¯”åˆ†æ: Flask vs FastAPI")
    print(f"{'='*60}")

    # è®¡ç®—æ€§èƒ½æå‡ç™¾åˆ†æ¯”
    throughput_improvement = (
        (fastapi_result.throughput - flask_result.throughput) / flask_result.throughput * 100
        if flask_result.throughput > 0 else 0
    )

    latency_improvement = (
        (flask_result.p99_latency - fastapi_result.p99_latency) / flask_result.p99_latency * 100
        if flask_result.p99_latency > 0 else 0
    )

    print(f"\nååé‡å¯¹æ¯”:")
    print(f"  Flask:  {flask_result.throughput:.2f} req/s")
    print(f"  FastAPI: {fastapi_result.throughput:.2f} req/s")
    print(f"  æå‡:   {throughput_improvement:+.1f}%")

    print(f"\nP99 å»¶è¿Ÿå¯¹æ¯”:")
    print(f"  Flask:  {flask_result.p99_latency*1000:.2f}ms")
    print(f"  FastAPI: {fastapi_result.p99_latency*1000:.2f}ms")
    print(f"  é™ä½:   {latency_improvement:+.1f}%")

    print(f"\næˆåŠŸç‡å¯¹æ¯”:")
    print(f"  Flask:  {flask_result.successful_requests}/{flask_result.total_requests} ({flask_result.successful_requests/flask_result.total_requests*100:.1f}%)")
    print(f"  FastAPI: {fastapi_result.successful_requests}/{fastapi_result.total_requests} ({fastapi_result.successful_requests/fastapi_result.total_requests*100:.1f}%)")

    print(f"{'='*60}\n")

    return {
        "throughput_improvement": throughput_improvement,
        "latency_improvement": latency_improvement
    }

# ========== ä¸»æµ‹è¯•æµç¨‹ ==========

async def run_full_benchmark():
    """
    è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶
    """
    print("\n" + "="*60)
    print("å¼‚æ­¥æ¡†æ¶æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*60)
    print(f"æµ‹è¯•é…ç½®:")
    print(f"  - å¹¶å‘è¯·æ±‚æ•°: {CONCURRENT_REQUESTS}")
    print(f"  - æ€»è¯·æ±‚æ•°: {TOTAL_REQUESTS}")
    print(f"  - è¶…æ—¶æ—¶é—´: {TIMEOUT}s")
    print(f"  - Flask URL: {FLASK_BASE_URL}")
    print(f"  - FastAPI URL: {FASTAPI_BASE_URL}")
    print("="*60)

    results = {
        "flask": {},
        "fastapi": {}
    }

    # æµ‹è¯•1: å¹¶å‘è¯·æ±‚æ€§èƒ½
    print("\n" + "="*60)
    print("æµ‹è¯•1: å¹¶å‘è¯·æ±‚æ€§èƒ½")
    print("="*60)

    try:
        results["flask"]["concurrent"] = await test_concurrent_requests(
            "Flask", FLASK_BASE_URL, "/api/health"
        )
    except Exception as e:
        print(f"âŒ Flask å¹¶å‘æµ‹è¯•å¤±è´¥: {e}")
        print("   æç¤º: è¯·ç¡®ä¿ Flask åº”ç”¨æ­£åœ¨è¿è¡Œ (python app.py)")

    try:
        results["fastapi"]["concurrent"] = await test_concurrent_requests(
            "FastAPI", FASTAPI_BASE_URL, "/api/health"
        )
    except Exception as e:
        print(f"âŒ FastAPI å¹¶å‘æµ‹è¯•å¤±è´¥: {e}")
        print("   æç¤º: è¯·ç¡®ä¿ FastAPI åº”ç”¨æ­£åœ¨è¿è¡Œ (python poc/fastapi_app.py)")

    # å¯¹æ¯”å¹¶å‘æµ‹è¯•ç»“æœ
    if "concurrent" in results["flask"] and "concurrent" in results["fastapi"]:
        compare_results(
            results["flask"]["concurrent"],
            results["fastapi"]["concurrent"]
        )

    # æµ‹è¯•2: SSE æµå¼å“åº”
    print("\n" + "="*60)
    print("æµ‹è¯•2: SSE æµå¼å“åº”æ€§èƒ½")
    print("="*60)

    try:
        results["flask"]["sse"] = await test_sse_stream(
            "Flask", FLASK_BASE_URL, "/api/analyze"
        )
    except Exception as e:
        print(f"âŒ Flask SSE æµ‹è¯•å¤±è´¥: {e}")

    try:
        results["fastapi"]["sse"] = await test_sse_stream(
            "FastAPI", FASTAPI_BASE_URL, "/api/analyze"
        )
    except Exception as e:
        print(f"âŒ FastAPI SSE æµ‹è¯•å¤±è´¥: {e}")

    # æµ‹è¯•3: å¼‚æ­¥æ“ä½œæ€§èƒ½ï¼ˆä»… FastAPIï¼‰
    print("\n" + "="*60)
    print("æµ‹è¯•3: å¼‚æ­¥æ“ä½œæ€§èƒ½ï¼ˆä»… FastAPIï¼‰")
    print("="*60)

    try:
        results["fastapi"]["async_ops"] = await test_async_operations(
            "FastAPI", FASTAPI_BASE_URL
        )
    except Exception as e:
        print(f"âŒ FastAPI å¼‚æ­¥æ“ä½œæµ‹è¯•å¤±è´¥: {e}")

    # ä¿å­˜ç»“æœ
    save_results(results)

    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° results/benchmark_results.json")
    print("="*60 + "\n")

    return results

def save_results(results: Dict):
    """
    ä¿å­˜æµ‹è¯•ç»“æœåˆ° JSON æ–‡ä»¶
    """
    import os
    os.makedirs("results", exist_ok=True)

    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    serializable_results = {
        "timestamp": time.time(),
        "flask": {
            key: value.to_dict() if isinstance(value, BenchmarkResult) else value
            for key, value in results.get("flask", {}).items()
        },
        "fastapi": {
            key: value.to_dict() if isinstance(value, BenchmarkResult) else value
            for key, value in results.get("fastapi", {}).items()
        }
    }

    with open("results/benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)

    print("\nâœ… ç»“æœå·²ä¿å­˜åˆ° results/benchmark_results.json")

# ========== å…¥å£ç‚¹ ==========

if __name__ == "__main__":
    print("\nğŸš€ å¯åŠ¨å¼‚æ­¥æ¡†æ¶æ€§èƒ½åŸºå‡†æµ‹è¯•...")

    # æ£€æŸ¥ä¾èµ–
    try:
        import httpx
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–: httpx")
        print("è¯·è¿è¡Œ: pip install httpx")
        exit(1)

    # è¿è¡Œæµ‹è¯•
    asyncio.run(run_full_benchmark())
